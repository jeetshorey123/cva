{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c042cd1",
   "metadata": {},
   "source": [
    "set a \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab522d73",
   "metadata": {},
   "source": [
    "***q1)Describe Kruskal's Algorithm, Explain Union Find Data Structure, used to detect Cycles while constructing MST.***\n",
    "answer)\n",
    "# Kruskal's Algorithm and Union-Find Data Structure\n",
    "\n",
    "## **Kruskal's Algorithm**\n",
    "Kruskal's Algorithm is a **greedy algorithm** used to find the **Minimum Spanning Tree (MST)** of a connected, weighted graph. It selects edges in ascending order of weight and adds them to the MST, ensuring no cycles are formed, until all vertices are connected.\n",
    "\n",
    "### **Steps:**\n",
    "1. Sort all edges of the graph in **non-decreasing order** of their weights.\n",
    "2. Initialize an empty set for the MST.\n",
    "3. Use a **Union-Find** data structure to keep track of which vertices belong to the same set (connected component).\n",
    "4. Iterate through the sorted edges:\n",
    "   - Add the edge to the MST if it does not form a cycle (i.e., the vertices are in different sets).\n",
    "   - Perform a **union** operation to combine the sets of the two vertices.\n",
    "5. Stop when the MST has \\( V - 1 \\) edges (where \\( V \\) is the number of vertices).\n",
    "\n",
    "---\n",
    "\n",
    "## **Union-Find Data Structure**\n",
    "The **Union-Find** (or Disjoint Set Union, DSU) is a data structure that efficiently manages a partition of a set into disjoint subsets. It supports two main operations:\n",
    "\n",
    "1. **Find(x):**  \n",
    "   Determines the representative (or root) of the set containing \\( x \\).  \n",
    "   Used to check if two elements are in the same subset.\n",
    "\n",
    "2. **Union(x, y):**  \n",
    "   Merges the subsets containing \\( x \\) and \\( y \\).\n",
    "\n",
    "### **Optimization Techniques:**\n",
    "1. **Path Compression:**  \n",
    "   During the Find operation, make nodes point directly to the root, flattening the structure and speeding up future operations.\n",
    "2. **Union by Rank/Size:**  \n",
    "   Attach the smaller tree under the larger tree to minimize height.\n",
    "\n",
    "---\n",
    "\n",
    "## **Cycle Detection in Kruskal’s Algorithm**\n",
    "Kruskal's Algorithm uses Union-Find to check whether adding an edge forms a cycle:\n",
    "\n",
    "1. For each edge \\( (u, v) \\), check if \\( u \\) and \\( v \\) belong to the same set using **Find**.\n",
    "2. If \\( \\text{Find}(u) == \\text{Find}(v) \\), adding the edge forms a cycle.\n",
    "3. Otherwise, perform **Union(u, v)** to combine their sets.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "### **Graph:**\n",
    "- **Edges:**  \n",
    "  \\( (1, 2, 4), (2, 3, 2), (1, 3, 5), (3, 4, 3) \\)\n",
    "\n",
    "### **Process:**\n",
    "\n",
    "1. **Sort edges:**  \n",
    "   \\( (2, 3, 2), (3, 4, 3), (1, 2, 4), (1, 3, 5) \\)\n",
    "\n",
    "2. **Add edges to MST:**\n",
    "   - Add \\( (2, 3, 2) \\), no cycle.\n",
    "   - Add \\( (3, 4, 3) \\), no cycle.\n",
    "   - Add \\( (1, 2, 4) \\), no cycle.\n",
    "   - Skip \\( (1, 3, 5) \\), as it forms a cycle.\n",
    "\n",
    "### **Resulting MST:**\n",
    "- **Edges:** \\( (2, 3, 2), (3, 4, 3), (1, 2, 4) \\)\n",
    "- **Total Weight:** \\( 9 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eba8f2",
   "metadata": {},
   "source": [
    "\n",
    "**q3a-Add this question: Practical on Time Complexity. (For the whole Question, we need to assume the data)**\n",
    "\n",
    "\n",
    "\n",
    "answer\n",
    "# Practical on Time Complexity\n",
    "\n",
    "### **Problem:**\n",
    "Analyze the time complexity of different algorithms for given operations. Assume a dataset and demonstrate the time complexities with examples.\n",
    "\n",
    "### **Steps:**\n",
    "\n",
    "1. **Assume a Dataset:**  \n",
    "   For this example, assume the following dataset:  \n",
    "   \\( \\text{Array} = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3] \\)\n",
    "\n",
    "2. **Perform the Following Operations:**\n",
    "   - **Search Operation:** Find an element in the array.\n",
    "   - **Sorting Operation:** Sort the array in ascending order.\n",
    "   - **Insertion Operation:** Insert an element at the correct position in a sorted array.\n",
    "\n",
    "3. **Analyze the Time Complexity:**\n",
    "   - **Search Operation:** Linear Search vs Binary Search.\n",
    "   - **Sorting Operation:** Compare different algorithms (e.g., Bubble Sort, Merge Sort, Quick Sort).\n",
    "   - **Insertion Operation:** Calculate the time required to insert an element in a sorted array.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "#### **1. Linear Search**\n",
    "- **Algorithm:** Traverse the array to find an element.\n",
    "- **Time Complexity:** \\( O(n) \\)\n",
    "\n",
    "#### **2. Sorting (Merge Sort)**\n",
    "- **Algorithm:** Recursively divide the array and merge sorted subarrays.\n",
    "- **Time Complexity:** \\( O(n \\log n) \\)\n",
    "\n",
    "#### **3. Insertion**\n",
    "- **Algorithm:** Locate the correct position for the new element and shift elements.\n",
    "- **Time Complexity:** \\( O(n) \\)\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing the Dataset:**\n",
    "\n",
    "```python\n",
    "# Assume this dataset\n",
    "array = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]\n",
    "\n",
    "# Demonstrate Linear Search\n",
    "key = 5\n",
    "index = -1\n",
    "for i in range(len(array)):\n",
    "    if array[i] == key:\n",
    "        index = i\n",
    "        break\n",
    "\n",
    "print(f\"Element {key} found at index {index}\")\n",
    "\n",
    "# Demonstrate Sorting\n",
    "sorted_array = sorted(array)\n",
    "print(f\"Sorted Array: {sorted_array}\")\n",
    "\n",
    "# Demonstrate Insertion\n",
    "new_element = 7\n",
    "sorted_array.append(new_element)\n",
    "sorted_array.sort()\n",
    "print(f\"Array after insertion: {sorted_array}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fae26f",
   "metadata": {},
   "source": [
    "# Q4-a: Difference Between 2-3 Tree, B-Tree, and Red-Black Tree\n",
    "\n",
    "## **1. 2-3 Tree:**\n",
    "- A **balanced search tree** where each node can have 2 or 3 children.\n",
    "- **All leaves** are at the same depth, ensuring balance.\n",
    "- Nodes contain **1 or 2 keys**:\n",
    "  - **2-node:** Contains 1 key and 2 children.\n",
    "  - **3-node:** Contains 2 keys and 3 children.\n",
    "- Provides efficient \\( O(\\log n) \\) time complexity for **search**, **insertion**, and **deletion**.\n",
    "- Balances itself by splitting nodes during insertions and merging nodes during deletions.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. B-Tree:**\n",
    "- A generalization of the 2-3 Tree, allowing each node to have **multiple keys**.\n",
    "- Each node can have between **\\( t-1 \\) and \\( 2t-1 \\) keys**, where \\( t \\) is the tree's minimum degree.\n",
    "- Ensures that the tree height remains low, even with a large number of keys.\n",
    "- Ideal for **databases** and **file systems**, as it minimizes disk accesses by storing multiple keys in each node.\n",
    "- Time complexity for **search**, **insertion**, and **deletion** is \\( O(\\log n) \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Red-Black Tree:**\n",
    "- A **binary search tree** with additional balancing properties:\n",
    "  - Each node is colored **red** or **black**.\n",
    "  - The tree satisfies certain properties to ensure balance:\n",
    "    1. Every node is either red or black.\n",
    "    2. The root is always black.\n",
    "    3. Red nodes cannot have red children (no two consecutive red nodes).\n",
    "    4. Every path from a node to its descendant leaves contains the same number of black nodes (black-height).\n",
    "- Balances itself through **rotations** and **recoloring** during insertions and deletions.\n",
    "- Time complexity for all operations is \\( O(\\log n) \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Comparison:**\n",
    "\n",
    "| Feature                  | **2-3 Tree**              | **B-Tree**                              | **Red-Black Tree**                     |\n",
    "|--------------------------|---------------------------|------------------------------------------|-----------------------------------------|\n",
    "| **Height**               | Low, balanced            | Low, balanced                            | Proportional to \\( \\log n \\)            |\n",
    "| **Insertion**            | Splits nodes if needed   | Redistributes keys or splits nodes       | Balances using rotations and recoloring |\n",
    "| **Deletion**             | May require merging nodes| Redistributes keys or merges nodes       | Balances using rotations and recoloring |\n",
    "| **Space Efficiency**     | Moderate                 | High (stores multiple keys per node)     | Moderate                                |\n",
    "| **Use Cases**            | Educational purposes     | Databases, file systems                  | Memory-efficient, dynamic datasets      |\n",
    "\n",
    "---\n",
    "\n",
    "# Q4-b: Insertions and Deletions\n",
    "\n",
    "## **1. Task Setup:**\n",
    "- Perform **insertion** and **deletion** operations on tree structures such as:\n",
    "  - **2-3 Tree**\n",
    "  - **B-Tree**\n",
    "  - **Red-Black Tree**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Insertion:**\n",
    "\n",
    "### **2-3 Tree:**\n",
    "- If a node becomes full:\n",
    "  1. Split the node into two nodes.\n",
    "  2. Push the middle key up to the parent node.\n",
    "- The tree remains balanced by ensuring all leaves are at the same depth.\n",
    "\n",
    "### **B-Tree:**\n",
    "- If a node becomes full:\n",
    "  1. Split the node into two nodes.\n",
    "  2. Redistribute keys between the nodes.\n",
    "  3. Move the middle key to the parent node.\n",
    "- Balances the tree by reducing its height.\n",
    "\n",
    "### **Red-Black Tree:**\n",
    "- Insert the node as in a binary search tree.\n",
    "- Perform **rotations** and **recoloring** to ensure the Red-Black Tree properties are maintained:\n",
    "  1. Fix consecutive red nodes.\n",
    "  2. Maintain black-height across all paths.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Deletion:**\n",
    "\n",
    "### **2-3 Tree:**\n",
    "- If a node becomes underfull:\n",
    "  1. Borrow a key from a sibling node.\n",
    "  2. Merge with a sibling if borrowing is not possible.\n",
    "- Ensures balance by maintaining the depth of all leaves.\n",
    "\n",
    "### **B-Tree:**\n",
    "- If a node becomes underfull:\n",
    "  1. Redistribute keys from sibling nodes.\n",
    "  2. Merge with a sibling if redistribution is not possible.\n",
    "- Ensures balance and minimizes height.\n",
    "\n",
    "### **Red-Black Tree:**\n",
    "- Remove the node as in a binary search tree.\n",
    "- Perform **rotations** and **recoloring** to restore the Red-Black Tree properties:\n",
    "  1. Fix violations of black-height.\n",
    "  2. Maintain the red-black property for nodes and paths.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Efficiency:**\n",
    "- All operations for **insertion** and **deletion** in 2-3 Tree, B-Tree, and Red-Black Tree have a time complexity of \\( O(\\log n) \\), ensuring efficient performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a8001",
   "metadata": {},
   "source": [
    "# Q6-a: Comparison of \\( F(n) = n! \\) and \\( G(n) = n^n \\)\n",
    "\n",
    "## **1. Definitions**\n",
    "\n",
    "### **Factorial Function (\\( F(n) = n! \\)):**\n",
    "- The factorial of \\( n \\), denoted as \\( n! \\), is the product of all positive integers from 1 to \\( n \\):\n",
    "  \\[\n",
    "  n! = n \\times (n-1) \\times (n-2) \\times \\dots \\times 1\n",
    "  \\]\n",
    "- Special case:\n",
    "  \\[\n",
    "  0! = 1\n",
    "  \\]\n",
    "- **Examples:**\n",
    "  - \\( 1! = 1 \\)\n",
    "  - \\( 2! = 2 \\times 1 = 2 \\)\n",
    "  - \\( 3! = 3 \\times 2 \\times 1 = 6 \\)\n",
    "  - \\( 4! = 4 \\times 3 \\times 2 \\times 1 = 24 \\)\n",
    "\n",
    "### **Exponential Power Function (\\( G(n) = n^n \\)):**\n",
    "- \\( n^n \\) represents \\( n \\) raised to the power of \\( n \\), which is equivalent to multiplying \\( n \\) by itself \\( n \\) times:\n",
    "  \\[\n",
    "  n^n = n \\times n \\times n \\times \\dots \\times n \\quad (\\text{total of \\( n \\) times})\n",
    "  \\]\n",
    "- **Examples:**\n",
    "  - \\( 1^1 = 1 \\)\n",
    "  - \\( 2^2 = 2 \\times 2 = 4 \\)\n",
    "  - \\( 3^3 = 3 \\times 3 \\times 3 = 27 \\)\n",
    "  - \\( 4^4 = 4 \\times 4 \\times 4 \\times 4 = 256 \\)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Growth Rate Comparison**\n",
    "\n",
    "### **Factorial Growth (\\( F(n) = n! \\)):**\n",
    "- Factorial grows **rapidly** as \\( n \\) increases.\n",
    "- Growth is slower compared to \\( n^n \\), but still very large.\n",
    "- **Asymptotic Behavior:**\n",
    "  \\[\n",
    "  F(n) = n! \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n \\quad \\text{(Stirling's Approximation)}\n",
    "  \\]\n",
    "\n",
    "### **Exponential Growth (\\( G(n) = n^n \\)):**\n",
    "- \\( n^n \\) grows much **faster** than \\( n! \\) because \\( n \\) is multiplied by itself \\( n \\) times.\n",
    "- For large \\( n \\), \\( n^n \\) dominates \\( n! \\).\n",
    "\n",
    "### **Comparison Table:**\n",
    "| \\( n \\) | \\( F(n) = n! \\) | \\( G(n) = n^n \\) |\n",
    "|--------|-----------------|-----------------|\n",
    "| 1      | 1               | 1               |\n",
    "| 2      | 2               | 4               |\n",
    "| 3      | 6               | 27              |\n",
    "| 4      | 24              | 256             |\n",
    "| 5      | 120             | 3,125           |\n",
    "| 6      | 720             | 46,656          |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Applications**\n",
    "\n",
    "### **Factorial (\\( n! \\)):**\n",
    "- Used in **combinatorics**:\n",
    "  - Number of ways to arrange \\( n \\) items: \\( n! \\).\n",
    "  - Example: Permutations and combinations.\n",
    "- Found in **probability theory** and **series expansions**:\n",
    "  - Taylor series expansions.\n",
    "  - Example: \\( e^x = \\sum_{n=0}^\\infty \\frac{x^n}{n!} \\).\n",
    "\n",
    "### **Exponential Power (\\( n^n \\)):**\n",
    "- Represents the number of ways to fill an \\( n \\times n \\) grid with \\( n \\) symbols.\n",
    "- Appears in problems involving large powers or recursive structures.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Visual Representation**\n",
    "\n",
    "### **Growth of \\( n! \\) and \\( n^n \\):**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Values of n\n",
    "n = np.arange(1, 8, 1)\n",
    "factorial = [np.math.factorial(i) for i in n]\n",
    "power = [i**i for i in n]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n, factorial, label='n!', marker='o')\n",
    "plt.plot(n, power, label='n^n', marker='x')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Growth of n! vs n^n')\n",
    "plt.yscale('log')  # Log scale to visualize the growth\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df115e",
   "metadata": {},
   "source": [
    "# Set 1\n",
    "\n",
    "## **Q1**\n",
    "\n",
    "### **Q1-a: Detecting Line Segment Intersection using BST**\n",
    "\n",
    "The **sweepline algorithm** combined with a **Binary Search Tree (BST)** offers an efficient approach for detecting line segment intersections. This method is widely used in computational geometry, such as in graphics and geometric modeling.\n",
    "\n",
    "#### **Steps:**\n",
    "\n",
    "1. **Sort the Events:**\n",
    "   - **Event Types:** Each event corresponds to either a **left endpoint** or **right endpoint** of a line segment.\n",
    "     - **Left Endpoint:** This marks the start of a segment.\n",
    "     - **Right Endpoint:** This marks the end of a segment.\n",
    "   - **Sorting Criteria:** \n",
    "     - Events are sorted first by the **x-coordinate**. If two events have the same x-coordinate, the **y-coordinate** is used to break ties.\n",
    "     - Events with the same x-coordinate are sorted based on the y-coordinate to handle vertical segments correctly.\n",
    "\n",
    "2. **Initialize the Sweep Line:**\n",
    "   - The **sweep line** starts from the leftmost x-coordinate of all events and moves to the right.\n",
    "   - At each step, the sweep line processes events one by one.\n",
    "   - The **active segments** are those that intersect with the current sweep line.\n",
    "\n",
    "3. **Use BST as the Status Structure:**\n",
    "   - The **Binary Search Tree (BST)** stores the active segments.\n",
    "   - The segments are arranged based on their **y-coordinates** at the sweep line, allowing efficient insertion, deletion, and neighbor checks.\n",
    "   - The tree structure allows for **logarithmic time complexity** when adding or removing segments, making it efficient.\n",
    "\n",
    "4. **Handle Events:**\n",
    "   - **Left Endpoint:** Add the segment to the BST.\n",
    "     - After insertion, check for potential intersections between the new segment and its neighbors in the BST (the adjacent segments).\n",
    "   - **Right Endpoint:** Remove the segment from the BST.\n",
    "     - Check for potential intersections between the neighbors that are left after removal.\n",
    "\n",
    "5. **Intersection Detection:**\n",
    "   - For each event, the algorithm checks the **two adjacent segments** in the BST (i.e., the segments directly above and below the current segment).\n",
    "   - If any of the adjacent segments intersect with the current segment, an intersection is detected.\n",
    "   - This method ensures that the algorithm only checks relevant pairs of segments that could intersect, reducing the time complexity.\n",
    "\n",
    "6. **Edge Cases:**\n",
    "   - **Vertical Segments:** Handle vertical segments specially. Since the x-coordinates of vertical segments are identical, it’s important to ensure they’re processed correctly during intersection detection.\n",
    "   - **Collinear or Overlapping Segments:** When segments are collinear or overlapping, the algorithm must distinguish between true intersections and overlaps at the same x and y coordinates.\n",
    "   - **Segment Order Maintenance:** The BST structure must properly maintain the order of segments at each sweep line position to ensure that adjacent segments are always correctly identified.\n",
    "   - **Complex Geometry:** Special care is needed for complex geometric scenarios, such as when segments meet at right angles or when segments overlap at their endpoints.\n",
    "\n",
    "7. **Advantages of the BST Approach:**\n",
    "   - **Efficient:** By using a BST to store and manage active segments, the algorithm reduces the number of pairwise comparisons and efficiently handles dynamic changes in the set of active segments.\n",
    "   - **Optimal Complexity:** The time complexity for insertion, deletion, and intersection checks is \\(O(\\log n)\\) for each event, leading to an overall time complexity of \\(O((n + k) \\log n)\\), where \\(k\\) is the number of intersections.\n",
    "\n",
    "8. **Use Cases:**\n",
    "   - **Computer Graphics:** Detecting object collisions, ray tracing, and geometric intersection problems.\n",
    "   - **Geographical Information Systems (GIS):** Detecting intersections between roads, rivers, and other geographical features.\n",
    "   - **Robotics:** Path planning and obstacle detection for robot navigation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q1-b: Single Linked List**\n",
    "\n",
    "A **single linked list** is a linear data structure where each node contains two fields:\n",
    "1. **Data**: Stores the actual value of the node.\n",
    "2. **Next Pointer**: A reference to the next node in the sequence.\n",
    "\n",
    "Unlike arrays, linked lists are dynamic, allowing for efficient insertion and deletion of elements, but with the trade-off of slower access times.\n",
    "\n",
    "#### **Operations:**\n",
    "\n",
    "1. **Insertion:**\n",
    "   - **At the Beginning:**\n",
    "     - Create a new node and point its `next` to the current head.\n",
    "     - Update the head to the new node.\n",
    "     - **Time Complexity:** \\( O(1) \\), since it requires only a few pointer updates.\n",
    "   \n",
    "   - **At the End:**\n",
    "     - Traverse the list to find the last node.\n",
    "     - Set the `next` pointer of the last node to the new node.\n",
    "     - **Time Complexity:** \\( O(n) \\) because the entire list needs to be traversed.\n",
    "\n",
    "   - **At a Specific Position:**\n",
    "     - Traverse the list to the desired position.\n",
    "     - Adjust the `next` pointers of the nodes around the insertion point.\n",
    "     - **Time Complexity:** \\( O(n) \\), since it requires traversal to the specific position.\n",
    "\n",
    "2. **Deletion:**\n",
    "   - **From the Beginning:**\n",
    "     - Update the head pointer to the second node in the list.\n",
    "     - **Time Complexity:** \\( O(1) \\), as it only involves updating the head pointer.\n",
    "   \n",
    "   - **From the End:**\n",
    "     - Traverse the list to find the second-to-last node.\n",
    "     - Set its `next` pointer to `None`.\n",
    "     - **Time Complexity:** \\( O(n) \\), as traversal of the entire list is required.\n",
    "\n",
    "   - **From a Specific Position:**\n",
    "     - Traverse the list to the target position.\n",
    "     - Update the `next` pointer of the node preceding the node to be deleted.\n",
    "     - **Time Complexity:** \\( O(n) \\), as traversal to the position is necessary.\n",
    "\n",
    "3. **Traversal:**\n",
    "   - Start from the head node, visiting each node sequentially.\n",
    "   - At each node, access its `data` and move to the next node.\n",
    "   - **Time Complexity:** \\( O(n) \\), as each node is visited once.\n",
    "\n",
    "4. **Search:**\n",
    "   - Starting from the head, traverse the list and check each node’s data to find the target element.\n",
    "   - If found, return the node or its index. If not found, return `None`.\n",
    "   - **Time Complexity:** \\( O(n) \\), since in the worst case, we may need to traverse the entire list.\n",
    "\n",
    "#### **Memory Usage:**\n",
    "- **Space Complexity:** \\( O(n) \\), since each node in the linked list requires additional memory to store the `next` pointer.\n",
    "- **No Wasted Space:** Unlike arrays, a linked list doesn’t require a predefined size, making it memory efficient for dynamic data.\n",
    "\n",
    "#### **Advantages:**\n",
    "- **Dynamic Size:** Linked lists grow and shrink dynamically without the need for resizing.\n",
    "- **Efficient Insertions/Deletions:** Inserting and deleting nodes at the beginning is quick and constant time, making it suitable for use in situations where such operations are frequent (e.g., implementing queues).\n",
    "\n",
    "#### **Disadvantages:**\n",
    "- **Sequential Access:** To access elements, the list must be traversed from the head, making random access slow compared to arrays.\n",
    "- **Extra Memory for Pointers:** Each node requires extra memory to store the `next` pointer, leading to higher overhead compared to arrays.\n",
    "\n",
    "#### **Use Cases:**\n",
    "- **Dynamic Data:** Suitable for situations where the size of the dataset changes frequently (e.g., real-time data processing, queues).\n",
    "- **Memory Efficiency:** Useful when the maximum size of the list is not known in advance.\n",
    "- **Implementing Data Structures:** Many data structures, such as queues, stacks, and adjacency lists in graphs, are often implemented using linked lists.\n",
    "\n",
    "#### **Summary of Time Complexity:**\n",
    "- **Insertion/Deletion at Beginning:** \\( O(1) \\).\n",
    "- **Insertion/Deletion at End:** \\( O(n) \\) (without a tail pointer), \\( O(1) \\) (with a tail pointer).\n",
    "- **Search/Deletion at Specific Position:** \\( O(n) \\).\n",
    "- **Traversal:** \\( O(n) \\).\n",
    "\n",
    "# **Q1-c: Time Complexity Analysis**\n",
    "\n",
    "### **Given time complexities:**\n",
    "1. \\( T(n) = n^2 \\) (quadratic growth).\n",
    "2. \\( T(n) = 2n^3 \\) (cubic growth).\n",
    "3. \\( T(n) = 10n^2 \\) (scaled quadratic growth).\n",
    "\n",
    "### **For \\( n = 100 \\):**\n",
    "- **\\( T(n) = n^2 \\):**\n",
    "  \\[\n",
    "  T(100) = 100^2 = 10,000\n",
    "  \\]\n",
    "  - The time complexity grows quadratically, resulting in 10,000 operations.\n",
    "\n",
    "- **\\( T(n) = 2n^3 \\):**\n",
    "  \\[\n",
    "  T(100) = 2 \\times 100^3 = 2 \\times 1,000,000 = 2,000,000\n",
    "  \\]\n",
    "  - The time complexity grows cubically, resulting in 2,000,000 operations.\n",
    "\n",
    "- **\\( T(n) = 10n^2 \\):**\n",
    "  \\[\n",
    "  T(100) = 10 \\times 100^2 = 10 \\times 10,000 = 100,000\n",
    "  \\]\n",
    "  - This is a scaled quadratic growth, resulting in 100,000 operations.\n",
    "\n",
    "### **Conclusion:**\n",
    "- At \\( n = 100 \\), the time complexity of \\( T(n) = 2n^3 \\) is the most computationally expensive because cubic growth outpaces quadratic growth.\n",
    "- \\( T(n) = 2n^3 \\) leads to 2,000,000 operations, while \\( T(n) = n^2 \\) and \\( T(n) = 10n^2 \\) only result in 10,000 and 100,000 operations, respectively.\n",
    "\n",
    "Thus, as \\( n \\) increases, cubic time complexities grow much faster than quadratic ones, making \\( T(n) = 2n^3 \\) the most costly for large \\( n \\).\n",
    "\n",
    "---\n",
    "### **Q1-d: Quick Sort Pseudocode**\n",
    "\n",
    "**Quick Sort** is a divide-and-conquer sorting algorithm that selects a pivot and partitions the array.\n",
    "\n",
    "#### **Pseudocode:**\n",
    "```python\n",
    "QuickSort(A, low, high):\n",
    "    if low < high:\n",
    "        pivot = Partition(A, low, high)   # Find pivot\n",
    "        QuickSort(A, low, pivot - 1)     # Recursively sort left subarray\n",
    "        QuickSort(A, pivot + 1, high)    # Recursively sort right subarray\n",
    "\n",
    "Partition(A, low, high):\n",
    "    pivot = A[high]    # Choose pivot as the last element\n",
    "    i = low - 1        # Initialize smaller element index\n",
    "    for j in range(low, high):\n",
    "        if A[j] <= pivot:\n",
    "            i += 1\n",
    "            swap(A[i], A[j])    # Swap smaller element to left\n",
    "    swap(A[i + 1], A[high])    # Place pivot at the correct position\n",
    "    return i + 1\n",
    "\n",
    "=### **Explanation:**\n",
    "\n",
    "#### **Partitioning:**\n",
    "- The `Partition` function selects the last element of the array as the pivot.\n",
    "- It then rearranges the elements such that those smaller than or equal to the pivot are placed to its left, and those greater than the pivot are placed to its right.\n",
    "- The pivot is placed in its correct sorted position, and the index of the pivot is returned.\n",
    "\n",
    "#### **Recursion:**\n",
    "- The `QuickSort` function is then called recursively on the left and right subarrays, which are divided around the pivot.\n",
    "\n",
    "#### **Pivot Selection:**\n",
    "- The pivot can be selected in various ways (e.g., random selection, first element, median of three), affecting the performance of the algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **Time Complexity:**\n",
    "\n",
    "#### **Best/Average Case:**\n",
    "- **Time Complexity:** \\( O(n \\log n) \\)\n",
    "  - The best case occurs when the pivot splits the array into nearly equal parts at each recursive step. This leads to \\( \\log n \\) levels of recursion, and each level processes \\( n \\) elements, resulting in \\( O(n \\log n) \\) time complexity.\n",
    "\n",
    "#### **Worst Case:**\n",
    "- **Time Complexity:** \\( O(n^2) \\)\n",
    "  - The worst case occurs when the pivot selection is poor (e.g., always selecting the smallest or largest element as the pivot). This results in unbalanced partitions and \\( n \\) levels of recursion, each of which processes \\( n \\) elements, leading to \\( O(n^2) \\) time complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Space Complexity:**\n",
    "- **Space Complexity:** \\( O(\\log n) \\)\n",
    "  - The space complexity comes from the recursion stack. In the best case, the recursion depth is \\( \\log n \\), so the space required is \\( O(\\log n) \\). In the worst case (unbalanced partitions), the recursion depth could be \\( O(n) \\), but with tail call optimization, the space complexity is typically \\( O(\\log n) \\).\n",
    "\n",
    "\n",
    "### **Q1-e: Kruskal's, Quick Find, Quick Union**\n",
    "\n",
    "### 1. Kruskal's Algorithm (Minimum Spanning Tree):\n",
    "\n",
    "Kruskal’s algorithm is used to find a Minimum Spanning Tree (MST) in a connected, undirected graph. The MST connects all the vertices in the graph with the minimum possible total edge weight, without forming any cycles.\n",
    "\n",
    "#### **Steps:**\n",
    "1. **Sort Edges:** Sort all the edges of the graph in non-decreasing order of their weights.\n",
    "2. **Initialize Union-Find Structure:** Use a union-find (also known as disjoint-set) data structure to keep track of the connected components (sets of vertices that are already connected).\n",
    "3. **Process Edges:**\n",
    "   - For each edge, check if it connects two vertices from different components.\n",
    "   - If they belong to different components, add the edge to the MST and merge the components.\n",
    "   - If the edge forms a cycle (i.e., both vertices are in the same component), discard the edge.\n",
    "4. **Stop:** Continue processing edges until \\( n-1 \\) edges are added to the MST, where \\( n \\) is the number of vertices in the graph.\n",
    "\n",
    "#### **Time Complexity:**\n",
    "- **Sorting Edges:** \\( O(E \\log E) \\), where \\( E \\) is the number of edges.\n",
    "- **Union-Find Operations:** \\( O(E \\log V) \\), where \\( V \\) is the number of vertices, since union-find operations can be made nearly constant using path compression and union by rank.\n",
    "- **Overall Time Complexity:** \\( O(E \\log E) \\), which simplifies to \\( O(E \\log V) \\) when \\( E \\) is approximately \\( O(V^2) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Quick Find:\n",
    "\n",
    "Quick Find is a union-find algorithm where each element is directly linked to the root of the set it belongs to. It is simple but inefficient for large graphs.\n",
    "\n",
    "#### **Operations:**\n",
    "- **Union:** To unite two sets, the algorithm changes all elements in one set to point to the root of the other set. This takes \\( O(n) \\) time in the worst case.\n",
    "- **Find:** Checking if two elements belong to the same set is very fast and takes \\( O(1) \\) time, as it only requires comparing their roots.\n",
    "\n",
    "#### **Time Complexity:**\n",
    "- **Union:** \\( O(n) \\) (because we may need to update all elements in one set).\n",
    "- **Find:** \\( O(1) \\) (only a root comparison).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quick Union:\n",
    "\n",
    "Quick Union improves upon Quick Find by storing elements as trees, where each element points to its parent. The root of a tree represents the set. Quick Union is more efficient than Quick Find for union operations but can still be slow for finding roots.\n",
    "\n",
    "#### **Operations:**\n",
    "- **Union:** To unite two sets, the root of one tree is linked to the root of the other tree. This operation takes \\( O(\\log n) \\) time in the average case, assuming the trees are balanced.\n",
    "- **Find:** To find the root of an element, the algorithm traverses the parent pointers until it reaches the root. This operation takes \\( O(\\log n) \\) time in the average case, assuming path compression is used.\n",
    "\n",
    "#### **Time Complexity:**\n",
    "- **Union:** \\( O(\\log n) \\) with path compression and balancing.\n",
    "- **Find:** \\( O(\\log n) \\) with path compression and balancing.\n",
    "\n",
    "#### **Path Compression:**\n",
    "- Path compression is a technique used in Quick Union to flatten the tree structure. When performing a Find operation, path compression makes each node along the path point directly to the root. This reduces the tree height, leading to faster future operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Time Complexities:**\n",
    "\n",
    "- **Kruskal’s Algorithm:**\n",
    "  - **Time Complexity:** \\( O(E \\log E) \\) or \\( O(E \\log V) \\), where \\( E \\) is the number of edges and \\( V \\) is the number of vertices.\n",
    "  - **Space Complexity:** \\( O(V) \\), as we need to store the union-find structure.\n",
    "\n",
    "- **Quick Find:**\n",
    "  - **Union Time:** \\( O(n) \\)\n",
    "  - **Find Time:** \\( O(1) \\)\n",
    "\n",
    "- **Quick Union:**\n",
    "  - **Union Time:** \\( O(\\log n) \\) (with path compression and balancing)\n",
    "  - **Find Time:** \\( O(\\log n) \\) (with path compression and balancing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee92fa8",
   "metadata": {},
   "source": [
    "\n",
    "## Q2)\n",
    "\n",
    "### a) Numerical Quick Find and Quick Union\n",
    "\n",
    "Let’s start by breaking down **Quick Find** and **Quick Union** with practical examples.\n",
    "\n",
    "#### Quick Find (Union-Find with Array)\n",
    "\n",
    "**Concept**:  \n",
    "In **Quick Find**, every element points to the root of the set it belongs to. The idea is that if two elements belong to the same set, their root values are the same.\n",
    "\n",
    "**Example**:  \n",
    "Suppose we have 10 elements (labeled 0 to 9), initially, each element is its own root:  \n",
    "Parent array: `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
    "\n",
    "1. **Find Operation**:  \n",
    "   - To check if two elements are in the same set, compare their roots. If the elements share the same root, they are in the same set.  \n",
    "   - Example: For elements 5 and 7, if `parent[5] == parent[7]`, then they are in the same set.\n",
    "\n",
    "2. **Union Operation**:  \n",
    "   - To connect two elements, make one element's root point to the other.  \n",
    "   - Example: If we want to union 4 and 7, we would update the parent array so that `parent[4] = parent[7]`.  \n",
    "     After performing a union of 4 and 7:  \n",
    "     Parent array: `[0, 1, 2, 3, 7, 5, 6, 7, 8, 9]`\n",
    "\n",
    "**Time Complexity**:  \n",
    "- **Find**: O(1) for each query, as each element directly points to its root.  \n",
    "- **Union**: O(n), because in the worst case, we may need to scan through the entire array to update the root of all elements in a set.\n",
    "\n",
    "#### Quick Union (Union-Find with Tree Structure)\n",
    "\n",
    "**Concept**:  \n",
    "In **Quick Union**, each element points to its parent rather than the root. The root of a set is the element that points to itself.\n",
    "\n",
    "**Example**:  \n",
    "Let’s initialize the array for 10 elements as follows (initially, each element is its own parent):  \n",
    "Parent array: `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
    "\n",
    "1. **Find Operation**:  \n",
    "   - To find the root of a set, we follow the parent pointers until we reach an element that points to itself.  \n",
    "   - Example: To find the root of 5, we follow the chain of parents: `parent[5] = 5`.\n",
    "\n",
    "2. **Union Operation**:  \n",
    "   - To union two sets, we link the root of one set to the root of the other set.  \n",
    "   - Example: To union 4 and 7, we would set `parent[4] = parent[7]`.  \n",
    "     After performing a union of 4 and 7:  \n",
    "     Parent array: `[0, 1, 2, 3, 7, 5, 6, 7, 8, 9]`\n",
    "\n",
    "**Time Complexity**:  \n",
    "- **Find**: O(log n) in the average case (depending on the depth of the tree).  \n",
    "- **Union**: O(log n) if the trees are balanced (after path compression).\n",
    "\n",
    "---\n",
    "\n",
    "### b) Practical Single Linked List - Intersection and Delete Implementation\n",
    "\n",
    "Let’s break down the **Single Linked List** implementation and operations like intersection and delete.\n",
    "\n",
    "#### Single Linked List Implementation:\n",
    "\n",
    "A **Single Linked List** is a linear data structure where each element (node) holds a reference to the next node in the list. Each node typically contains two parts:\n",
    "- **Data**: The value stored in the node.\n",
    "- **Next**: A pointer/reference to the next node in the list.\n",
    "\n",
    "#### Code for Intersection of Two Linked Lists:\n",
    "\n",
    "The **intersection** of two linked lists means finding the node where they first intersect, i.e., when they have the same reference (memory address).\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.next = None\n",
    "\n",
    "def get_intersection_node(head1, head2):\n",
    "    if not head1 or not head2:\n",
    "        return None\n",
    "    \n",
    "    # Get the lengths of both lists\n",
    "    len1 = get_length(head1)\n",
    "    len2 = get_length(head2)\n",
    "    \n",
    "    # Move the pointer of the longer list to align with the shorter list\n",
    "    if len1 > len2:\n",
    "        for _ in range(len1 - len2):\n",
    "            head1 = head1.next\n",
    "    else:\n",
    "        for _ in range(len2 - len1):\n",
    "            head2 = head2.next\n",
    "    \n",
    "    # Move both pointers and check for intersection\n",
    "    while head1 and head2:\n",
    "        if head1 == head2:\n",
    "            return head1  # Intersection node found\n",
    "        head1 = head1.next\n",
    "        head2 = head2.next\n",
    "    \n",
    "    return None  # No intersection\n",
    "\n",
    "def get_length(head):\n",
    "    length = 0\n",
    "    while head:\n",
    "        length += 1\n",
    "        head = head.next\n",
    "    return length\n",
    "\n",
    "Code for Deleting a Node from a Linked List:\n",
    "The delete operation in a singly linked list requires you to find the node and update the previous node's next pointer to skip over the node being deleted.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def delete_node(head, key):\n",
    "    # If the node to be deleted is the head node\n",
    "    if head and head.data == key:\n",
    "        head = head.next\n",
    "        return head\n",
    "    \n",
    "    current = head\n",
    "    while current and current.next:\n",
    "        if current.next.data == key:\n",
    "            current.next = current.next.next  # Skip the node to delete it\n",
    "            return head\n",
    "        current = current.next\n",
    "    return head  # Node with key not found\n",
    "\n",
    "###c) Advantages and Disadvantages of Binary Trees\n",
    "    \n",
    "\n",
    "   # Advantages and Disadvantages of Binary Trees\n",
    "\n",
    "Binary trees are a fundamental data structure that plays a crucial role in computer science, offering several advantages and disadvantages depending on their type and how they are utilized.\n",
    "\n",
    "## Advantages of Binary Trees\n",
    "\n",
    "### 1. **Efficient Searching**\n",
    "- **Binary Search Tree (BST)**: A special type of binary tree where for each node, the left child’s value is less than the parent’s value, and the right child’s value is greater. This structure allows for **efficient searching**, insertion, and deletion of elements.\n",
    "- **Time Complexity**: In a balanced binary search tree, operations such as searching, insertion, and deletion take O(log n) time on average, which makes them faster than linear data structures like arrays and linked lists, where these operations might take O(n) time.\n",
    "  \n",
    "  Example:\n",
    "  - In a balanced BST, to find an element, we start from the root and choose to move either left or right based on the value comparison. This reduces the search space by half at each level.\n",
    "\n",
    "### 2. **Hierarchical Data Representation**\n",
    "- Binary trees naturally represent **hierarchical structures** like file systems, organization charts, and decision trees.\n",
    "- They help in organizing data in a way that reflects hierarchical relationships where each node can have at most two child nodes (left and right), making it ideal for representing systems with parent-child relationships.\n",
    "\n",
    "  Example:\n",
    "  - A directory structure on a computer can be represented using a binary tree, where each node represents a folder, and the child nodes represent subfolders or files.\n",
    "\n",
    "### 3. **Balanced Trees Guarantee Efficient Operations**\n",
    "- **Self-balancing binary trees** like AVL trees or Red-Black trees maintain balance during insertion and deletion operations, ensuring that the height of the tree remains O(log n).\n",
    "- **Time Complexity**: For balanced binary trees, most operations like searching, insertion, and deletion take O(log n) time, even for large datasets. This guarantees consistent fast performance.\n",
    "\n",
    "  Example:\n",
    "  - AVL trees automatically perform rotations to ensure balance, and Red-Black trees apply color changes to nodes to maintain balance during insertions and deletions.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of Binary Trees\n",
    "\n",
    "### 1. **Unbalanced Trees**\n",
    "- If a binary tree is not balanced (i.e., the height difference between the left and right subtrees of a node is large), it can degrade to a **skewed tree**.\n",
    "- A skewed tree is essentially a **linked list**, where each node only has one child. In this case, the time complexity for searching, insertion, and deletion becomes **O(n)**, which is inefficient compared to balanced trees.\n",
    "  \n",
    "  Example:\n",
    "  - If elements are inserted in ascending or descending order into a binary search tree without balancing, the tree will become skewed, with all nodes appearing as right or left children of the previous node.\n",
    "\n",
    "### 2. **Memory Overhead**\n",
    "- Each node in a binary tree requires extra memory to store pointers (left and right). In addition to storing the value of the node, these pointers increase the memory usage of the data structure.\n",
    "- **Memory Usage**: In binary trees, every node needs space for at least two pointers (left and right), which adds memory overhead, especially for large datasets.\n",
    "  \n",
    "  Example:\n",
    "  - For large-scale applications, such as databases or file systems, the extra memory overhead for maintaining pointers can become significant, especially if the dataset has a large number of nodes.\n",
    "\n",
    "### 3. **Complexity of Balancing**\n",
    "- For self-balancing binary trees (like **AVL** or **Red-Black trees**), maintaining balance adds additional **complexity** to the implementation.\n",
    "- **Rotations and Color Changes**: In AVL trees, the tree may require rotations after every insertion or deletion to maintain balance. In Red-Black trees, color changes and rotations are required, which makes the implementation more complex compared to simple binary search trees.\n",
    "  \n",
    "  Example:\n",
    "  - AVL trees use **single and double rotations** to keep the tree balanced. This requires careful tracking of the height of nodes, adding complexity to the operations.\n",
    "  - Red-Black trees perform **color-flipping and rotations** to maintain their properties, which can increase the complexity of insertion and deletion operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While binary trees provide significant advantages in terms of efficient searching, hierarchical data representation, and fast operations in balanced cases, they also come with challenges such as memory overhead and the risk of poor performance when unbalanced. Additionally, maintaining balance in self-balancing binary trees requires added complexity in the implementation. When used appropriately, binary trees can be a highly effective data structure, but their limitations need to be considered, especially in systems with large or dynamic datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b149a",
   "metadata": {},
   "source": [
    "# d)Binary Tree and Comparison Between All Types of Trees\n",
    "\n",
    "## Binary Tree Pseudocode\n",
    "\n",
    "A binary tree is a tree data structure where each node has at most two children: a left child and a right child.\n",
    "\n",
    "```python\n",
    "# Node structure for a binary tree\n",
    "class TreeNode:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# Insertion of data in a binary tree (no specific order)\n",
    "def insert(root, data):\n",
    "    if root is None:\n",
    "        return TreeNode(data)\n",
    "    else:\n",
    "        if data < root.data:\n",
    "            root.left = insert(root.left, data)\n",
    "        else:\n",
    "            root.right = insert(root.right, data)\n",
    "    return root\n",
    "\n",
    "# In-order Traversal (Left, Root, Right)\n",
    "def inorder(root):\n",
    "    if root:\n",
    "        inorder(root.left)\n",
    "        print(root.data)\n",
    "        inorder(root.right)\n",
    "\n",
    "Comparison of Tree Types\n",
    "1. Binary Tree\n",
    "Definition: A tree where each node has at most two children.\n",
    "\n",
    "Structure: No specific order or arrangement of nodes.\n",
    "\n",
    "Use Case: General-purpose tree representation.\n",
    "\n",
    "2. Binary Search Tree (BST)\n",
    "Definition: A special type of binary tree where:\n",
    "\n",
    "The left child’s value is less than the parent node's value.\n",
    "\n",
    "The right child’s value is greater than the parent node's value.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Efficient search, insertion, and deletion in O(log n) (when balanced).\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Can become unbalanced and degrade to O(n) operations.\n",
    "\n",
    "3. AVL Tree\n",
    "Definition: A self-balancing binary search tree where the heights of two child subtrees of any node differ by at most 1.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Guarantees O(log n) operations due to strict balancing.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Complex implementation.\n",
    "\n",
    "Requires rotations to maintain balance.\n",
    "\n",
    "4. Red-Black Tree\n",
    "Definition: A self-balancing binary search tree with an extra attribute for each node, called color (red or black).\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Easier to implement than AVL trees.\n",
    "\n",
    "Guarantees O(log n) operations.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "More complex than a basic binary search tree.\n",
    "\n",
    "Not as strictly balanced as an AVL tree.\n",
    "\n",
    "5. B-Tree\n",
    "Definition: A self-balancing tree designed for disk storage, where nodes can contain multiple keys.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Efficient for large-scale data storage, especially in disk-based systems.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "More complex implementation.\n",
    "\n",
    "Requires management of multiple keys per node.\n",
    "\n",
    "Summary of Tree Types\n",
    "Tree Type\tMax Children\tStructure\tAdvantages\tDisadvantages\n",
    "Binary Tree\t2\tNo specific order\tSimple, flexible structure\tNo efficiency guarantees\n",
    "BST\t2\tOrdered (left < root < right)\tEfficient search, insertion, deletion\tCan become unbalanced\n",
    "AVL Tree\t2\tBalanced BST\tAlways balanced, O(log n) operations\tComplex rotations, harder to implement\n",
    "Red-Black Tree\t2\tBalanced BST with colors\tEasier than AVL, O(log n) operations\tLess strictly balanced than AVL\n",
    "B-Tree\tMultiple\tMulti-key nodes\tDisk-efficient, large-scale data\tComplex, multi-key node management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27174424",
   "metadata": {},
   "source": [
    "# e)Red-Black Tree Operations: Intersection, Deletion, and Rotation\n",
    "\n",
    "A Red-Black Tree (RBT) is a self-balancing binary search tree with an additional **color attribute** (either red or black) for each node. It ensures logarithmic time complexity for search, insertion, and deletion operations by maintaining specific properties.\n",
    "\n",
    "---\n",
    "\n",
    "## Red-Black Tree Properties\n",
    "\n",
    "1. Every node is either **red** or **black**.\n",
    "2. The **root** is always black.\n",
    "3. Every leaf (**NIL node**) is black.\n",
    "4. If a red node has children, both must be black (**no two red nodes can be adjacent**).\n",
    "5. Every path from a node to its descendant NIL nodes must have the **same number of black nodes**.\n",
    "\n",
    "---\n",
    "\n",
    "## Intersection in Red-Black Tree\n",
    "\n",
    "Finding the intersection of two Red-Black Trees involves identifying common elements between the two trees. This can be achieved by performing simultaneous searches on both trees.\n",
    "\n",
    "### Example Implementation:\n",
    "```python\n",
    "def intersection(rbt1, rbt2):\n",
    "    result = []\n",
    "\n",
    "    def inorder_traversal(root, tree):\n",
    "        if root:\n",
    "            inorder_traversal(root.left, tree)\n",
    "            if search(tree, root.data):  # Assuming a `search` function exists\n",
    "                result.append(root.data)\n",
    "            inorder_traversal(root.right, tree)\n",
    "\n",
    "    inorder_traversal(rbt1.root, rbt2)\n",
    "    return result\n",
    "\n",
    "\n",
    "Deletion in Red-Black Tree\n",
    "Deletion in a Red-Black Tree involves two key steps:\n",
    "\n",
    "Standard BST Deletion: Remove the node as in a regular binary search tree.\n",
    "\n",
    "Fixing Violations: Ensure Red-Black Tree properties are restored by performing rotations and color adjustments.\n",
    "\n",
    "Deletion Pseudocode:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def delete(root, key):\n",
    "    # Standard BST deletion\n",
    "    if key < root.data:\n",
    "        root.left = delete(root.left, key)\n",
    "    elif key > root.data:\n",
    "        root.right = delete(root.right, key)\n",
    "    else:\n",
    "        if root.left and root.right:\n",
    "            successor = find_min(root.right)  # Find minimum in the right subtree\n",
    "            root.data = successor.data\n",
    "            root.right = delete(root.right, successor.data)\n",
    "        else:\n",
    "            # Replace the node with its child or NIL\n",
    "            root = root.left if root.left else root.right\n",
    "\n",
    "    # Fixing violations after deletion (recoloring and rotations)\n",
    "    if root and root.color == 'red':\n",
    "        # Perform the necessary rotations and color fixes here\n",
    "        pass\n",
    "    return root\n",
    "Rotation in Red-Black Tree\n",
    "Rotations are used to maintain Red-Black Tree properties, especially after insertions or deletions. There are two types of rotations:\n",
    "\n",
    "1. Left Rotation\n",
    "A left rotation makes the right child of a node become the new parent.\n",
    "\n",
    "Pseudocode for Left Rotation:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def left_rotate(x):\n",
    "    y = x.right  # Identify the right child\n",
    "    x.right = y.left\n",
    "    if y.left != NIL:\n",
    "        y.left.parent = x\n",
    "    y.parent = x.parent\n",
    "    if x.parent == NIL:\n",
    "        root = y\n",
    "    else:\n",
    "        if x == x.parent.left:\n",
    "            x.parent.left = y\n",
    "        else:\n",
    "            x.parent.right = y\n",
    "    y.left = x\n",
    "    x.parent = y\n",
    "2. Right Rotation\n",
    "A right rotation is similar but rotates around a node to the right, making the left child the new parent.\n",
    "\n",
    "Pseudocode for Right Rotation:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def right_rotate(x):\n",
    "    y = x.left  # Identify the left child\n",
    "    x.left = y.right\n",
    "    if y.right != NIL:\n",
    "        y.right.parent = x\n",
    "    y.parent = x.parent\n",
    "    if x.parent == NIL:\n",
    "        root = y\n",
    "    else:\n",
    "        if x == x.parent.right:\n",
    "            x.parent.right = y\n",
    "        else:\n",
    "            x.parent.left = y\n",
    "    y.right = x\n",
    "    x.parent = y\n",
    "Summary of Operations\n",
    "Operation\tDescription\n",
    "Intersection\tFind common elements by traversing and comparing nodes of two trees.\n",
    "Deletion\tStandard BST removal followed by fixing violations using rotations/colors.\n",
    "Rotation\tBalances the tree by rearranging parent-child relationships.\n",
    "\n",
    "Left Rotation: Makes the right child the parent.\n",
    "\n",
    "Right Rotation: Makes the left child the parent.\n",
    "\n",
    "Copy\n",
    "Edit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733d2e3",
   "metadata": {},
   "source": [
    "# q3)Hash Tables: Intersection, Deletion, Search, and Robin-Karp Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "## a) Hash Table Implementation\n",
    "\n",
    "A **Hash Table** is a data structure that stores key-value pairs. It uses a **hash function** to map keys to indices in an array. Collisions are handled through techniques such as linear probing, chaining, or double hashing.\n",
    "\n",
    "### Basic Operations\n",
    "\n",
    "#### Pseudocode:\n",
    "```python\n",
    "class HashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size  # Initialize table with None\n",
    "    \n",
    "    def hash_function(self, key):\n",
    "        return key % self.size  # Simple modulo hash function\n",
    "    \n",
    "    def insert(self, key, value):\n",
    "        index = self.hash_function(key)\n",
    "        self.table[index] = value  # In case of collision, use probing\n",
    "    \n",
    "    def search(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        return self.table[index]  # Handle collisions appropriately\n",
    "    \n",
    "    def delete(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        self.table[index] = None  # Remove the element\n",
    "        Example:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "ht = HashTable(10)\n",
    "ht.insert(1, 'A')\n",
    "ht.insert(11, 'B')  # Collision occurs, needs handling\n",
    "print(ht.search(1))  # Output: 'A'\n",
    "ht.delete(1)\n",
    "print(ht.search(1))  # Output: None\n",
    "b) Robin-Karp Algorithm\n",
    "The Rabin-Karp Algorithm is a string-searching algorithm that uses hashing to find substrings in a text. It computes a hash value for the substring (pattern) and compares it with the hash of the text's windows.\n",
    "\n",
    "Steps:\n",
    "Compute the hash value of the pattern and the first window of text.\n",
    "\n",
    "Compare the hash values:\n",
    "\n",
    "If they match, compare the characters to confirm.\n",
    "\n",
    "Slide the window by one character:\n",
    "\n",
    "Update the hash value for the new window.\n",
    "\n",
    "Repeat until the end of the text.\n",
    "\n",
    "Pseudocode:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def rabin_karp(text, pattern):\n",
    "    n = len(text)\n",
    "    m = len(pattern)\n",
    "    base = 256  # Number of possible characters (extended ASCII)\n",
    "    prime = 101  # A large prime number\n",
    "    \n",
    "    # Compute hash for the pattern and first window of the text\n",
    "    pattern_hash = 0\n",
    "    text_hash = 0\n",
    "    for i in range(m):\n",
    "        pattern_hash = (base * pattern_hash + ord(pattern[i])) % prime\n",
    "        text_hash = (base * text_hash + ord(text[i])) % prime\n",
    "    \n",
    "    # Slide the window over the text\n",
    "    for i in range(n - m + 1):\n",
    "        # Check if hash values match\n",
    "        if pattern_hash == text_hash:\n",
    "            if text[i:i+m] == pattern:  # Verify by comparing characters\n",
    "                print(\"Pattern found at index\", i)\n",
    "        \n",
    "        # Update the hash value for the next window\n",
    "        if i < n - m:\n",
    "            text_hash = (base * (text_hash - ord(text[i]) * (base ** (m - 1)))) + ord(text[i + m])\n",
    "            text_hash %= prime  # Ensure the hash value remains positive\n",
    "Summary\n",
    "Hash Table Operations:\n",
    "Operation\tDescription\tComplexity\n",
    "Insert\tAdd a key-value pair.\tO(1)\n",
    "Search\tRetrieve a value by its key.\tO(1)\n",
    "Delete\tRemove a key-value pair.\tO(1)\n",
    "\n",
    "Rabin-Karp Algorithm:\n",
    "Step\tDescription\tComplexity\n",
    "Precompute hashes\tCompute pattern and first text hash.\tO(m)\n",
    "Sliding window\tUpdate hash for each window.\tO(n-m+1)\n",
    "Verify match\tCompare characters when hashes match.\tO(m)\n",
    "Overall\tEfficient for small patterns.\tO(n+m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a914e",
   "metadata": {},
   "source": [
    "# b)Numerical Hash Table with Linear Probing and Probabilities\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Linear Probing** is a collision resolution technique used in hash tables. When a collision occurs (i.e., two keys hash to the same index), linear probing searches the next available slot in a sequential manner until an empty slot is found. This process ensures that all elements are stored within the hash table without requiring additional structures like linked lists.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. **Hash Function**\n",
    "The hash function is used to map keys to indices in the hash table. A simple hash function can be:\n",
    "```python\n",
    "index = key % table_size\n",
    "This hash function maps keys uniformly to indices within the table size.\n",
    "\n",
    "2. Load Factor (α)\n",
    "The load factor is a measure of how full the hash table is. It is defined as:\n",
    "\n",
    "𝛼\n",
    "=\n",
    "Number of elements in the table\n",
    "Total slots in the table\n",
    "α= \n",
    "Total slots in the table\n",
    "Number of elements in the table\n",
    "​\n",
    " \n",
    "A higher load factor means the table is more occupied, increasing the probability of collisions.\n",
    "\n",
    "Typical threshold values for load factors are:\n",
    "\n",
    "0.5–0.75: Efficient operation.\n",
    "\n",
    "> 0.75: Resize the table to maintain performance.\n",
    "\n",
    "Linear Probing: How It Works\n",
    "When inserting an element:\n",
    "\n",
    "Compute the hash index using the hash function.\n",
    "\n",
    "Check if the slot is occupied:\n",
    "\n",
    "If empty, place the element.\n",
    "\n",
    "If occupied, move to the next slot (linear probing).\n",
    "\n",
    "Continue the search (wrapping around using modulo operation) until an empty slot is found.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "class HashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size\n",
    "    \n",
    "    def hash_function(self, key):\n",
    "        return key % self.size\n",
    "    \n",
    "    def insert(self, key, value):\n",
    "        index = self.hash_function(key)\n",
    "        while self.table[index] is not None:  # Linear probing\n",
    "            index = (index + 1) % self.size  # Move to the next slot\n",
    "        self.table[index] = value\n",
    "    \n",
    "    def search(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        while self.table[index] is not None:\n",
    "            if self.table[index][0] == key:\n",
    "                return self.table[index][1]\n",
    "            index = (index + 1) % self.size\n",
    "        return None\n",
    "Probability of Collision\n",
    "1. Initial Probability\n",
    "The probability of a collision when inserting an element into a hash table is directly related to the load factor. Initially:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "collision\n",
    ")\n",
    "=\n",
    "𝛼\n",
    "=\n",
    "Number of occupied slots\n",
    "Total slots\n",
    "P(collision)=α= \n",
    "Total slots\n",
    "Number of occupied slots\n",
    "​\n",
    " \n",
    "2. Successive Probes\n",
    "With linear probing, if a collision occurs, subsequent slots are probed until an empty slot is found. The probability of additional collisions depends on:\n",
    "\n",
    "The current load factor.\n",
    "\n",
    "The uniformity of the hash function.\n",
    "\n",
    "Expected Probes for Insertion/Search:\n",
    "Average case: \n",
    "1\n",
    "2\n",
    "(\n",
    "1\n",
    "+\n",
    "1\n",
    "1\n",
    "−\n",
    "𝛼\n",
    ")\n",
    "2\n",
    "1\n",
    "​\n",
    " (1+ \n",
    "1−α\n",
    "1\n",
    "​\n",
    " )\n",
    "\n",
    "Worst case: The hash table becomes full, requiring \n",
    "𝑂\n",
    "(\n",
    "𝑛\n",
    ")\n",
    "O(n) probes.\n",
    "\n",
    "Advantages of Linear Probing\n",
    "Efficient Use of Space: All elements are stored within the hash table, avoiding external structures.\n",
    "\n",
    "Simplicity: Easy to implement compared to other collision resolution methods.\n",
    "\n",
    "Disadvantages of Linear Probing\n",
    "Clustering: Elements tend to cluster in contiguous blocks, increasing the number of probes.\n",
    "\n",
    "Performance Degrades with High Load Factor: As \n",
    "𝛼\n",
    "α approaches 1, the number of probes required increases significantly.\n",
    "\n",
    "Strategies to Minimize Collisions\n",
    "Good Hash Function: Ensure uniform distribution of keys.\n",
    "\n",
    "Control Load Factor: Resize the table when \n",
    "𝛼\n",
    ">\n",
    "0.75\n",
    "α>0.75.\n",
    "\n",
    "Double Hashing: Use a secondary hash function to resolve collisions.\n",
    "\n",
    "Applications\n",
    "Database Indexing: Efficient lookup of keys in large datasets.\n",
    "\n",
    "Caching Systems: Fast retrieval of frequently accessed data.\n",
    "\n",
    "Symbol Tables in Compilers: Storing variable names and their associated information.\n",
    "\n",
    "Conclusion\n",
    "Linear probing is a simple and effective method for handling collisions in hash tables. However, careful attention must be given to the hash function and load factor to maintain performance. Understanding the probability of collisions and their impact is crucial for optimizing hash table usage in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72845b80",
   "metadata": {},
   "source": [
    "# Detailed Explanation of Asymptotic Notation and Data Structures\n",
    "\n",
    "## Big-O Notation: Order of Growth in Asymptotic Notation\n",
    "\n",
    "### **Definition**\n",
    "Big-O notation is used to describe the upper bound of the growth rate of an algorithm's time or space complexity. It characterizes the worst-case scenario, helping understand how an algorithm scales with input size.\n",
    "\n",
    "### **Common Big-O Notations**\n",
    "1. **O(1)**: Constant time. Execution time is independent of input size.\n",
    "   - Example: Accessing an element in an array.\n",
    "2. **O(log n)**: Logarithmic time. Execution time grows logarithmically with input size.\n",
    "   - Example: Binary search.\n",
    "3. **O(n)**: Linear time. Execution time grows linearly with input size.\n",
    "   - Example: Traversing an array.\n",
    "4. **O(n²)**: Quadratic time. Execution time grows quadratically with input size.\n",
    "   - Example: Nested loops.\n",
    "5. **O(2ⁿ)**: Exponential time. Execution time doubles with each additional input.\n",
    "   - Example: Recursive algorithms solving the Tower of Hanoi.\n",
    "\n",
    "### **Justifying Big-O**\n",
    "Given a function \\( f(n) \\), if \\( f(n) = O(g(n)) \\), it means that for sufficiently large \\( n \\), there exists a constant \\( C \\) such that:\n",
    "\\[\n",
    "f(n) \\leq C \\cdot g(n)\n",
    "\\]\n",
    "- Example: \\( f(n) = 3n² + 5n \\)\n",
    "  - Dominant term: \\( 3n² \\)\n",
    "  - \\( f(n) = O(n²) \\) because for large \\( n \\), \\( n² \\) dominates the other terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Queue Data Structure\n",
    "\n",
    "### **Definition**\n",
    "A queue is a linear data structure that follows the **FIFO** (First In, First Out) principle. Elements are added at the back and removed from the front.\n",
    "\n",
    "### **Operations**\n",
    "1. **Enqueue**: Add an element to the back of the queue.\n",
    "2. **Dequeue**: Remove the element from the front of the queue.\n",
    "3. **Peek**: View the front element without removing it.\n",
    "4. **IsEmpty**: Check if the queue is empty.\n",
    "\n",
    "### **Applications**\n",
    "- **Task Scheduling**: Managing processes in operating systems.\n",
    "- **Asynchronous Data Handling**: Queues for I/O operations.\n",
    "- **Graph Traversal**: Used in Breadth-First Search (BFS).\n",
    "\n",
    "---\n",
    "\n",
    "## 2-3 Tree: Height and Maximum Nodes\n",
    "\n",
    "### **Definition**\n",
    "A 2-3 tree is a balanced search tree where each node can have either 2 or 3 children. It maintains balance by ensuring all leaves are at the same depth.\n",
    "\n",
    "### **Properties**\n",
    "1. All nodes have either 2 or 3 children.\n",
    "2. All leaves are at the same level.\n",
    "\n",
    "### **Maximum Height**\n",
    "The height \\( h \\) of a 2-3 tree with \\( n \\) nodes is:\n",
    "\\[\n",
    "h = O(\\log_3 n)\n",
    "\\]\n",
    "\n",
    "### **Maximum Nodes**\n",
    "A 2-3 tree with height \\( h \\) can have at most:\n",
    "\\[\n",
    "n = 3^h - 1\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Red-Black Tree Operations\n",
    "\n",
    "### **Intersection of Two Red-Black Trees**\n",
    "1. Perform an **inorder traversal** on both trees.\n",
    "2. Compare the values of nodes to find common elements.\n",
    "3. Add matching elements to the result.\n",
    "\n",
    "**Pseudocode**:\n",
    "```python\n",
    "def inorder_traversal(root, tree, result):\n",
    "    if root:\n",
    "        inorder_traversal(root.left, tree, result)  # Left subtree\n",
    "        if search(tree, root.data):  # Check for matching element\n",
    "            result.append(root.data)\n",
    "        inorder_traversal(root.right, tree, result)  # Right subtree\n",
    "Deletion in a Red-Black Tree\n",
    "Standard BST Deletion: Remove the node like a regular BST.\n",
    "\n",
    "Fixing Violations:\n",
    "\n",
    "Recoloring: Adjust the color of nodes.\n",
    "\n",
    "Rotations: Perform left or right rotations to balance the tree.\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def delete(root, key):\n",
    "    if key < root.data:\n",
    "        root.left = delete(root.left, key)\n",
    "    elif key > root.data:\n",
    "        root.right = delete(root.right, key)\n",
    "    else:\n",
    "        if root.left and root.right:\n",
    "            successor = find_min(root.right)\n",
    "            root.data = successor.data\n",
    "            root.right = delete(root.right, successor.data)\n",
    "        else:\n",
    "            root = root.left if root.left else root.right\n",
    "\n",
    "    # Fix Red-Black properties here (e.g., rotations and recoloring)\n",
    "    return root\n",
    "Rotations in a Red-Black Tree\n",
    "Left Rotation\n",
    "The right child of the current node becomes the new root of the subtree.\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def left_rotate(x):\n",
    "    y = x.right\n",
    "    x.right = y.left\n",
    "    if y.left != NIL:\n",
    "        y.left.parent = x\n",
    "    y.parent = x.parent\n",
    "    if x.parent == NIL:\n",
    "        root = y\n",
    "    else:\n",
    "        if x == x.parent.left:\n",
    "            x.parent.left = y\n",
    "        else:\n",
    "            x.parent.right = y\n",
    "    y.left = x\n",
    "    x.parent = y\n",
    "Right Rotation\n",
    "The left child of the current node becomes the new root of the subtree.\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def right_rotate(x):\n",
    "    y = x.left\n",
    "    x.left = y.right\n",
    "    if y.right != NIL:\n",
    "        y.right.parent = x\n",
    "    y.parent = x.parent\n",
    "    if x.parent == NIL:\n",
    "        root = y\n",
    "    else:\n",
    "        if x == x.parent.right:\n",
    "            x.parent.right = y\n",
    "        else:\n",
    "            x.parent.left = y\n",
    "    y.right = x\n",
    "    x.parent = y\n",
    "These explanations and algorithms provide a comprehensive overview of the topics and operations, suitable for deeper understanding and practical implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f4c13",
   "metadata": {},
   "source": [
    "q3) Hash Tables: Intersection, Deletion, Search, and Robin-Karp Algorithm\n",
    "a) Hash Table Implementation\n",
    "A hash table is a data structure that allows for efficient searching, insertion, and deletion of elements. It stores key-value pairs and uses a hash function to map keys to indices in an array. Collisions occur when two keys hash to the same index, and they are handled through techniques like linear probing.\n",
    "Let’s discuss insertion, search, and deletion operations in a hash table.\n",
    "Hash Table Operations:\n",
    "1.\tInsertion: Insert a key-value pair by calculating the hash index using a hash function and placing it at that index.\n",
    "2.\tSearch: To search for a key, compute the hash and check if the value at the index matches the key.\n",
    "3.\tDeletion: Similar to search, but after finding the key, remove it and handle any collisions that may have occurred.\n",
    "Pseudocode for Hash Table Operations:\n",
    "python\n",
    "CopyEdit\n",
    "class HashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size\n",
    "    \n",
    "    def hash_function(self, key):\n",
    "        return key % self.size  # Simple modulo hash function\n",
    "    \n",
    "    def insert(self, key, value):\n",
    "        index = self.hash_function(key)\n",
    "        # Handle collision using linear probing\n",
    "        while self.table[index] is not None:\n",
    "            index = (index + 1) % self.size  # Linear probing\n",
    "        self.table[index] = value\n",
    "    \n",
    "    def search(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        # Handle collision using linear probing\n",
    "        while self.table[index] is not None:\n",
    "            if self.table[index] == key:\n",
    "                return index\n",
    "            index = (index + 1) % self.size  # Linear probing\n",
    "        return None  # Key not found\n",
    "    \n",
    "    def delete(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        while self.table[index] is not None:\n",
    "            if self.table[index] == key:\n",
    "                self.table[index] = None\n",
    "                return True\n",
    "            index = (index + 1) % self.size  # Linear probing\n",
    "        return False  # Key not found\n",
    "Example:\n",
    "python\n",
    "CopyEdit\n",
    "ht = HashTable(10)\n",
    "ht.insert(1, 'A')\n",
    "ht.insert(11, 'B')  # Collision occurs\n",
    "ht.search(1)  # Returns 'A'\n",
    "ht.delete(1)\n",
    "ht.search(1)  # Returns None\n",
    "Robin-Karp Algorithm (for String Matching):\n",
    "The Rabin-Karp algorithm is used to search for a pattern in a text by computing hash values of the text and pattern. If the hash values match, it performs a more detailed comparison of the pattern and the text at that position.\n",
    "Pseudocode:\n",
    "python\n",
    "CopyEdit\n",
    "def rabin_karp(text, pattern):\n",
    "    n = len(text)\n",
    "    m = len(pattern)\n",
    "    base = 256  # ASCII characters\n",
    "    prime = 101  # A large prime number\n",
    "    \n",
    "    # Calculate hash values for pattern and the first window of the text\n",
    "    pattern_hash = 0\n",
    "    text_hash = 0\n",
    "    for i in range(m):\n",
    "        pattern_hash = (base * pattern_hash + ord(pattern[i])) % prime\n",
    "        text_hash = (base * text_hash + ord(text[i])) % prime\n",
    "    \n",
    "    # Slide the window over the text\n",
    "    for i in range(n - m + 1):\n",
    "        if pattern_hash == text_hash:\n",
    "            if text[i:i+m] == pattern:\n",
    "                print(\"Pattern found at index\", i)\n",
    "        \n",
    "        # Update hash value for next window using the previous hash\n",
    "        if i < n - m:\n",
    "            text_hash = (base * (text_hash - ord(text[i]) * (base ** (m - 1)))) + ord(text[i + m])\n",
    "            text_hash %= prime\n",
    "________________________________________\n",
    "b) Numerical Hash Table/Linear Probing/Probabilities\n",
    "Linear Probing is a collision resolution strategy used in hash tables. When a collision occurs, we move sequentially through the array until we find an empty spot.\n",
    "Load Factor:\n",
    "The load factor (α) is the ratio of the number of elements in the hash table to the total number of slots. It affects performance, as higher load factors increase the chance of collisions.\n",
    "Collision Probability:\n",
    "For a hash table with n elements and a size of m, the probability of collision using linear probing can be approximated as:\n",
    "P(collision)=1−(m−1m)nP(collision) = 1 - \\left( \\frac{m-1}{m} \\right)^nP(collision)=1−(mm−1)n\n",
    "As the load factor increases, the probability of collision increases, and thus the performance of the hash table degrades.\n",
    "________________________________________\n",
    "q6) Order of Growth in Asymptotic Notation\n",
    "a) Big-O Notation\n",
    "Big-O notation is used to describe the upper bound of an algorithm’s time complexity, indicating the worst-case scenario as the input size grows large. It is essential for analyzing the efficiency of algorithms.\n",
    "For instance, consider an algorithm with time complexity f(n)=O(g(n))f(n) = O(g(n))f(n)=O(g(n)). This means there exists a constant CCC such that for sufficiently large nnn:\n",
    "f(n)≤C×g(n)f(n) \\leq C \\times g(n)f(n)≤C×g(n) \n",
    "•\tExample: If f(n)=3n2+5nf(n) = 3n^2 + 5nf(n)=3n2+5n, we can say that f(n)=O(n2)f(n) = O(n^2)f(n)=O(n2), because for large nnn, n2n^2n2 dominates the other terms.\n",
    "Justifying Big-O:\n",
    "To prove f(n)=O(n2)f(n) = O(n^2)f(n)=O(n2), we need to find a constant CCC such that:\n",
    "3n2+5n≤C×n23n^2 + 5n \\leq C \\times n^23n2+5n≤C×n2 \n",
    "For sufficiently large nnn, we can choose C=4C = 4C=4, because 3n2+5n≤4n23n^2 + 5n \\leq 4n^23n2+5n≤4n2.\n",
    "b) Queue Data Structure\n",
    "A Queue is a linear data structure that follows the FIFO (First In, First Out) principle. It operates like a real-world queue where the first person to enter the queue is the first one to leave.\n",
    "Queue operations include:\n",
    "1.\tEnqueue: Add an element to the rear of the queue.\n",
    "2.\tDequeue: Remove an element from the front of the queue.\n",
    "3.\tPeek: View the front element without removing it.\n",
    "4.\tIsEmpty: Check if the queue is empty.\n",
    "Applications:\n",
    "•\tScheduling tasks in operating systems.\n",
    "•\tBreadth-First Search (BFS) in graph traversal.\n",
    "________________________________________\n",
    "2-3 Tree Maximum Height and Maximum Nodes\n",
    "A 2-3 Tree is a balanced search tree where each internal node has either two or three children, and all leaves are at the same level.\n",
    "•\tMaximum Height: The maximum height of a 2-3 tree with nnn nodes is O(log⁡3n)O(\\log_3 n)O(log3n).\n",
    "•\tMaximum Nodes: For a 2-3 tree with height hhh, the maximum number of nodes is\n",
    "4o mini\n",
    "ChatGPT said:\n",
    "given by:\n",
    "nmax=3h+1−12n_{max} = \\frac{3^{h+1} - 1}{2}nmax=23h+1−1 \n",
    "________________________________________\n",
    "Would you like a specific algorithm explained in more detail?\n",
    "\n",
    "Q7a) Heap Sort: Numerical, Theory, Code, and Design\n",
    "________________________________________\n",
    "What is Heap Sort?\n",
    "Heap Sort is a comparison-based sorting algorithm that leverages a binary heap data structure. It has two main phases:\n",
    "1.\tBuilding a Max-Heap: Transform the input array into a max-heap, where the largest element is at the root.\n",
    "2.\tExtracting the Maximum: Repeatedly swap the root with the last element of the heap, reduce the size of the heap by one, and re-heapify to maintain the max-heap property.\n",
    "Time Complexity:\n",
    "•\tBuilding the heap: O(n)O(n)\n",
    "•\tHeapify: O(log⁡n)O(\\log n) per element\n",
    "•\tTotal: O(nlog⁡n)O(n \\log n)\n",
    "________________________________________\n",
    "Properties of Heap\n",
    "1.\tBinary Heap Structure:\n",
    "o\tEach level of the heap is filled left-to-right.\n",
    "o\tA complete binary tree representation.\n",
    "2.\tMax-Heap Property:\n",
    "o\tEvery parent node is greater than or equal to its children.\n",
    "3.\tMin-Heap Property:\n",
    "o\tEvery parent node is less than or equal to its children.\n",
    "________________________________________\n",
    "Heap Sort Algorithm\n",
    "1.\tBuild a Max-Heap:\n",
    "o\tStart from the last non-leaf node (index ⌊n/2⌋−1\\lfloor n/2 \\rfloor - 1) and perform heapify for each node up to the root.\n",
    "2.\tSort the Array:\n",
    "o\tSwap the root (maximum element) with the last element in the array.\n",
    "o\tReduce the size of the heap by one.\n",
    "o\tPerform heapify on the root.\n",
    "________________________________________\n",
    "Example: Sort Array [4, 10, 3, 5, 1]\n",
    "1.\tInput: [4, 10, 3, 5, 1]\n",
    "2.\tBuild Max-Heap:\n",
    "o\tStart from the last non-leaf node (index 1).\n",
    "o\tHeapify: Adjust the subtree rooted at index 1: [10, 4, 3, 5, 1].\n",
    "o\tHeapify index 0: [10, 5, 3, 4, 1].\n",
    "3.\tHeap Sort:\n",
    "o\tSwap the root (10) with the last element: [1, 5, 3, 4, 10].\n",
    "o\tHeapify: [5, 4, 3, 1, 10].\n",
    "o\tSwap the root (5) with the second last element: [1, 4, 3, 5, 10].\n",
    "o\tHeapify: [4, 1, 3, 5, 10].\n",
    "o\tRepeat until sorted: [1, 3, 4, 5, 10].\n",
    "________________________________________\n",
    "Pseudocode for Heap Sort\n",
    "def heapify(arr, n, i):\n",
    "    largest = i  # Initialize largest as root\n",
    "    left = 2 * i + 1  # Left child\n",
    "    right = 2 * i + 2  # Right child\n",
    "    \n",
    "    # Check if left child exists and is greater than root\n",
    "    if left < n and arr[left] > arr[largest]:\n",
    "        largest = left\n",
    "\n",
    "    # Check if right child exists and is greater than root\n",
    "    if right < n and arr[right] > arr[largest]:\n",
    "        largest = right\n",
    "\n",
    "    # If largest is not root, swap and continue heapifying\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]\n",
    "        heapify(arr, n, largest)\n",
    "\n",
    "def heap_sort(arr):\n",
    "    n = len(arr)\n",
    "    \n",
    "    # Build max heap\n",
    "    for i in range(n // 2 - 1, -1, -1):\n",
    "        heapify(arr, n, i)\n",
    "\n",
    "    # Extract elements one by one\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]  # Swap\n",
    "        heapify(arr, i, 0)\n",
    "Example Code Execution:\n",
    "arr = [4, 10, 3, 5, 1]\n",
    "heap_sort(arr)\n",
    "print(\"Sorted array:\", arr)\n",
    "Output:\n",
    "Sorted array: [1, 3, 4, 5, 10]\n",
    "________________________________________\n",
    "Heap Map Design\n",
    "A heap map visualizes the structure of a binary heap:\n",
    "•\tRepresent the array as a tree.\n",
    "•\tEach index ii corresponds to:\n",
    "o\tLeft child: 2i+12i + 1\n",
    "o\tRight child: 2i+22i + 2\n",
    "Example Heap Map for [10, 5, 3, 4, 1]:\n",
    "          10\n",
    "        /    \\\n",
    "      5        3\n",
    "     / \\\n",
    "    4   1\n",
    "________________________________________\n",
    "Q7b) BST Traversal: Preorder, Postorder, and Inorder\n",
    "Binary Search Tree (BST) Traversals\n",
    "1.\tPreorder Traversal:\n",
    "o\tVisit the root node.\n",
    "o\tRecursively traverse the left subtree.\n",
    "o\tRecursively traverse the right subtree.\n",
    "2.\tInorder Traversal:\n",
    "o\tRecursively traverse the left subtree.\n",
    "o\tVisit the root node.\n",
    "o\tRecursively traverse the right subtree.\n",
    "3.\tPostorder Traversal:\n",
    "o\tRecursively traverse the left subtree.\n",
    "o\tRecursively traverse the right subtree.\n",
    "o\tVisit the root node.\n",
    "________________________________________\n",
    "Example Tree:\n",
    "        10\n",
    "       /  \\\n",
    "      5    15\n",
    "     / \\     \\\n",
    "    2   7     20\n",
    "Traversals:\n",
    "•\tPreorder: [10, 5, 2, 7, 15, 20]\n",
    "•\tInorder: [2, 5, 7, 10, 15, 20]\n",
    "•\tPostorder: [2, 7, 5, 20, 15, 10]\n",
    "________________________________________\n",
    "Pseudocode for Traversals\n",
    "# Preorder Traversal\n",
    "def preorder(node):\n",
    "    if node:\n",
    "        print(node.data, end=' ')\n",
    "        preorder(node.left)\n",
    "        preorder(node.right)\n",
    "\n",
    "# Inorder Traversal\n",
    "def inorder(node):\n",
    "    if node:\n",
    "        inorder(node.left)\n",
    "        print(node.data, end=' ')\n",
    "        inorder(node.right)\n",
    "\n",
    "# Postorder Traversal\n",
    "def postorder(node):\n",
    "    if node:\n",
    "        postorder(node.left)\n",
    "        postorder(node.right)\n",
    "        print(node.data, end=' ')\n",
    "________________________________________\n",
    "Storing Traversals in a Hash Table\n",
    "We can store the result of each traversal in a hash table where the keys are the traversal types and the values are the traversal orders.\n",
    "Example:\n",
    "traversals = {\n",
    "    \"preorder\": [10, 5, 2, 7, 15, 20],\n",
    "    \"inorder\": [2, 5, 7, 10, 15, 20],\n",
    "    \"postorder\": [2, 7, 5, 20, 15, 10]\n",
    "}\n",
    "Code:\n",
    "def store_traversals(root):\n",
    "    traversals = {}\n",
    "    traversals[\"preorder\"] = []\n",
    "    traversals[\"inorder\"] = []\n",
    "    traversals[\"postorder\"] = []\n",
    "\n",
    "    def preorder(node):\n",
    "        if node:\n",
    "            traversals[\"preorder\"].append(node.data)\n",
    "            preorder(node.left)\n",
    "            preorder(node.right)\n",
    "\n",
    "    def inorder(node):\n",
    "        if node:\n",
    "            inorder(node.left)\n",
    "            traversals[\"inorder\"].append(node.data)\n",
    "            inorder(node.right)\n",
    "\n",
    "    def postorder(node):\n",
    "        if node:\n",
    "            postorder(node.left)\n",
    "            postorder(node.right)\n",
    "            traversals[\"postorder\"].append(node.data)\n",
    "\n",
    "    preorder(root)\n",
    "    inorder(root)\n",
    "    postorder(root)\n",
    "\n",
    "    return traversals\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
